{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, pandas, numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovProcess:\n",
    "    def __init__(self, state_space: list, transition_probability_matrix: numpy.ndarray) -> None:\n",
    "        '''Markov Proces is a tuple (S, P), where S is a state space and P is a transition probability matrix.'''\n",
    "        self.state_space = state_space\n",
    "        self.transition_probability_matrix = transition_probability_matrix\n",
    "\n",
    "        self.validate_inputs()\n",
    "\n",
    "    def validate_inputs(self):\n",
    "\n",
    "        ### Dimensions\n",
    "        assert len(self.state_space) == len(set(self.state_space)), 'Some state names are not unique!'\n",
    "        self.num_states = len(self.state_space)\n",
    "\n",
    "        # Transition prob shape\n",
    "        assert self.transition_probability_matrix.shape[0] == self.num_states\n",
    "        assert self.transition_probability_matrix.shape[0] == self.transition_probability_matrix.shape[1]\n",
    "\n",
    "        # Transition prob entries\n",
    "        assert numpy.all(self.transition_probability_matrix <= 1)\n",
    "        assert numpy.all(self.transition_probability_matrix >= 0)\n",
    "        probs_sum = numpy.sum(self.transition_probability_matrix, axis=1)\n",
    "        assert numpy.all(numpy.isclose(probs_sum, 1) + numpy.isclose(probs_sum, 0)), 'Sum of probabilities must be 0 (for terminal states) or 1 in each row!'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = [f's{i}' for i in range(4)]\n",
    "transition_probability_matrix = numpy.array(\n",
    "    [\n",
    "        [5/6, 1/6, 0,     0],\n",
    "        [0 ,  1/6, 4/6, 1/6],\n",
    "        [0 ,  0,   3/4, 1/4],\n",
    "        [1,   0,   0,     0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "markov_process = MarkovProcess(state_space=state_space, transition_probability_matrix=transition_probability_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergodicity TODO\n",
    "\n",
    "$$\n",
    "P(S_{j}) = \\sum_{i} P(S_{j} | S_{i}) * P(S_{i}) \\quad \\forall j=1,...,N \\\\\n",
    "P^{T} \\bold{p} = \\bold{p}\\\\\n",
    "$$\n",
    "\n",
    "This implies that $\\bold{p}$ is the eignevector of $P$ corresponding to the eigenvalue of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigevalues, eigenvectors = numpy.linalg.eigh(transition_probability_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.66666667,  0.16666667,  0.75      ,  1.5       ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigevalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.where(numpy.isclose(eigevalues, 1)) # TODO: why is there no unit eigenvalue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit_eigenvalue_idx = numpy.where(numpy.isclose(eigevalues, 1))[0][0]\n",
    "# eigenvectors[unit_eigenvalue_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Reward Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovRewardProcess:\n",
    "    def __init__(\n",
    "        self, reward_vector: numpy.ndarray, discount_factor: numbers.Real,\n",
    "        markov_process: MarkovProcess = None, state_space: list = None, transition_probability_matrix: numpy.ndarray = None, \n",
    "    ) -> None:\n",
    "        '''\n",
    "        A Markov Reward Process (MRP) is a tuple (S, P, R, gamma). Note: there is no action space!\n",
    "        '''\n",
    "        \n",
    "        assert (markov_process is not None) or (state_space is not None and transition_probability_matrix is not None)\n",
    "        if markov_process is None:\n",
    "            self._markov_process = MarkovProcess(\n",
    "                state_space=state_space, transition_probability_matrix=transition_probability_matrix\n",
    "            ) # NOTE: this validates state space and TPM\n",
    "        self.state_space = self._markov_process.state_space\n",
    "        self.num_states = self._markov_process.num_states\n",
    "        self.transition_probability_matrix = self._markov_process.transition_probability_matrix\n",
    "        self.reward_vector = reward_vector\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.validate_inputs()\n",
    "\n",
    "        # Results\n",
    "        self.value_function = None\n",
    "\n",
    "    def validate_inputs(self):\n",
    "\n",
    "        # Reward vector shape\n",
    "        if len(self.reward_vector.shape) == 2:\n",
    "            assert self.reward_vector.shape[1] == 1\n",
    "            self.reward_vector = self.reward_vector.reshape(-1,)\n",
    "        assert len(self.reward_vector) == self.num_states\n",
    "\n",
    "        ### Discount factor\n",
    "        assert 0 <= self.discount_factor <= 1\n",
    "\n",
    "    def calculate_value_function(self, convert_to_series: bool = False):\n",
    "        identity_matrix = numpy.diag(numpy.ones(self.num_states))\n",
    "        inv_matrix = numpy.linalg.inv(identity_matrix - self.discount_factor * self.transition_probability_matrix)\n",
    "        self.value_function = inv_matrix @ self.reward_vector\n",
    "        if convert_to_series:\n",
    "            self.value_function = pandas.Series(data=self.value_function, index=self.state_space)\n",
    "        return self.value_function                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s0    147.339983\n",
       "s1     36.699917\n",
       "s2    -39.526185\n",
       "s3    -11.596010\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_vector = numpy.array(\n",
    "    [70, 50, -20, -100]\n",
    ")\n",
    "discount_factor = 0.6\n",
    "\n",
    "\n",
    "mrp = MarkovRewardProcess(\n",
    "    state_space=state_space, transition_probability_matrix=transition_probability_matrix, reward_vector=reward_vector, discount_factor=discount_factor\n",
    ")\n",
    "mrp.calculate_value_function(convert_to_series=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(\n",
    "        self, state_space: list, action_space: list,\n",
    "        transition_probability_matrix: numpy.ndarray, reward_matrix: numpy.ndarray, discount_factor: numbers.Real\n",
    "    ) -> None:\n",
    "        '''Environment is a Markov Decision Process without policy.'''\n",
    "        \n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.transition_probability_matrix = transition_probability_matrix\n",
    "        self.reward_matrix = reward_matrix\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.validate_inputs()\n",
    "\n",
    "    def validate_inputs(self):\n",
    "\n",
    "        ### State space shape\n",
    "        assert len(self.state_space) == len(set(self.state_space)), 'Some state names are not unique!'\n",
    "        self.num_states = len(self.state_space)\n",
    "\n",
    "        ### Action space shape\n",
    "        assert len(self.action_space) == len(set(self.action_space)), 'Some state names are not unique!'\n",
    "        self.num_actions = len(self.action_space)\n",
    "\n",
    "        # Transition prob shape: num_states x num_states x num_actions\n",
    "        assert len(self.transition_probability_matrix.shape) == 3\n",
    "        assert self.transition_probability_matrix.shape[0] == self.num_states\n",
    "        assert self.transition_probability_matrix.shape[1] == self.num_states\n",
    "        assert self.transition_probability_matrix.shape[2] == self.num_actions\n",
    "\n",
    "        # Reward matrix shape: num_states x num_states x num_actions OR num_states x num_actions\n",
    "        assert len(self.reward_matrix.shape) in [2, 3]\n",
    "        assert self.reward_matrix.shape[0] == self.num_states\n",
    "        if len(self.reward_matrix.shape) == 3:\n",
    "            assert self.reward_matrix.shape[1] == self.num_states\n",
    "            assert self.reward_matrix.shape[2] == self.num_actions\n",
    "        else:\n",
    "            assert self.reward_matrix.shape[1] == self.num_actions\n",
    "\n",
    "        # Transition prob entries\n",
    "        assert numpy.all(self.transition_probability_matrix <= 1)\n",
    "        assert numpy.all(self.transition_probability_matrix >= 0)\n",
    "        probs_sum = numpy.sum(self.transition_probability_matrix, axis=1)\n",
    "        assert numpy.all(\n",
    "            numpy.isclose(probs_sum, 1) + numpy.isclose(probs_sum, 0)\n",
    "        ), 'Sum of probabilities must be 0 (for terminal states) or 1 in each row!'\n",
    "\n",
    "        ### Discount factor\n",
    "        assert 0 <= self.discount_factor <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_transition_probability_matrix = numpy.array([\n",
    "    [ # action 0\n",
    "        [0, 5/6, 1/6, 0  ],\n",
    "        [0, 1/6, 4/6, 1/6],\n",
    "        [0, 0,   2/3, 1/3],\n",
    "        [0, 0,   0,   1  ],\n",
    "    ],\n",
    "    [ # action 1\n",
    "        [5/6, 1/6, 0,    0  ],\n",
    "        [0,   4/5, 1/5,  0  ],\n",
    "        [0,   0,   3/4,  1/4],\n",
    "        [0,   0,   0,    1  ],\n",
    "    ],\n",
    "    [ # action 2\n",
    "        [1, 0, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "    ]\n",
    "])\n",
    "env_transition_probability_matrix = numpy.swapaxes(env_transition_probability_matrix, axis1=0, axis2=1)\n",
    "env_transition_probability_matrix = numpy.swapaxes(env_transition_probability_matrix, axis1=1, axis2=2)\n",
    "env_transition_probability_matrix.shape # number of initial states x number of arriving states x number of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_reward_matrix_3d = numpy.array([\n",
    "    [ # action 0\n",
    "        [numpy.nan, 100,       100,       numpy.nan],\n",
    "        [numpy.nan, 50,        50,        50       ],\n",
    "        [numpy.nan, numpy.nan, 10,        10       ],\n",
    "        [numpy.nan, numpy.nan, numpy.nan, 0        ],\n",
    "    ],\n",
    "    [ # action 1\n",
    "        [70,        70,        numpy.nan, numpy.nan],\n",
    "        [numpy.nan, 20,        20,        numpy.nan],\n",
    "        [numpy.nan, numpy.nan, -20,       -20      ],\n",
    "        [numpy.nan, numpy.nan, numpy.nan, -30      ],\n",
    "    ],\n",
    "    [ # action 2 (renovate)\n",
    "        [0,    numpy.nan, numpy.nan, numpy.nan],\n",
    "        [-50,  numpy.nan, numpy.nan, numpy.nan],\n",
    "        [-90,  numpy.nan, numpy.nan, numpy.nan],\n",
    "        [-100, numpy.nan, numpy.nan, numpy.nan],\n",
    "    ]\n",
    "])\n",
    "env_reward_matrix_3d = numpy.swapaxes(env_reward_matrix_3d, axis1=0, axis2=1)\n",
    "env_reward_matrix_3d = numpy.swapaxes(env_reward_matrix_3d, axis1=1, axis2=2)\n",
    "env_reward_matrix_3d.shape # number of actions x number of initial states x number of arriving states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: we can represent rewards in 2D becase the depend only on initial states and actions, they do not depend on arriving states\n",
    "env_reward_matrix_2d = numpy.array([\n",
    "    [100, 50, 10, 0],  # action 0\n",
    "    [70, 20, -20, -30], # action 1\n",
    "    [0, -50, -90, -100],  # action 2 (renovate)\n",
    "])\n",
    "env_reward_matrix_2d = numpy.swapaxes(env_reward_matrix_2d, axis1=0, axis2=1)\n",
    "\n",
    "env_reward_matrix_2d.shape # number of initial states x number of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_state_space = [f's{i}' for i in range(4)]\n",
    "env_action_space = [f's{i}' for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment(\n",
    "    state_space=env_state_space, \n",
    "    action_space=env_action_space,\n",
    "    transition_probability_matrix=env_transition_probability_matrix,\n",
    "    reward_matrix=env_reward_matrix_2d,\n",
    "    discount_factor=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_all3d = Environment(\n",
    "    state_space=env_state_space, \n",
    "    action_space=env_action_space,\n",
    "    transition_probability_matrix=env_transition_probability_matrix,\n",
    "    reward_matrix=env_reward_matrix_3d,\n",
    "    discount_factor=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess:\n",
    "    def __init__(self, environment: Environment, policy: numpy.ndarray) -> None:\n",
    "        self.environment = environment\n",
    "        self.policy = policy\n",
    "        self.deterministic_policy: bool = None \n",
    "        \n",
    "        self.validate_inputs()\n",
    "    \n",
    "    def validate_inputs(self):\n",
    "\n",
    "        assert len(self.policy.shape) <= 2\n",
    "        assert self.policy.shape[0] == self.environment.num_states\n",
    "\n",
    "        if len(self.policy.shape) == 1:\n",
    "            self.deterministic_policy = True\n",
    "        elif self.policy.shape[1] == 1:\n",
    "            self.policy = self.policy.reshape(-1,)\n",
    "            self.deterministic_policy = True\n",
    "        else:\n",
    "            self.deterministic_policy = False\n",
    "        \n",
    "        if not self.deterministic_policy:\n",
    "            assert self.policy.shape[1] == self.environment.num_actions\n",
    "            probs_sum = numpy.sum(self.policy, axis=1)\n",
    "            assert numpy.all(\n",
    "                numpy.isclose(probs_sum, 1) + numpy.isclose(probs_sum, 0)\n",
    "            ), 'In your policy, the sum of action probabilities must be 0 (for terminal states) or 1 (in each row)!'\n",
    "\n",
    "\n",
    "    ### TODO\n",
    "    def _calculate_policy_tpm(self):\n",
    "        pass\n",
    "\n",
    "    def _calculate_policy_rewards(self):\n",
    "        pass\n",
    "\n",
    "    def calculate_policy_value_function(self, convert_to_series: bool = False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = numpy.array([1, 0, 1, 2])\n",
    "mdp = MarkovDecisionProcess(environment=environment, policy=policy)\n",
    "mdp.deterministic_policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
